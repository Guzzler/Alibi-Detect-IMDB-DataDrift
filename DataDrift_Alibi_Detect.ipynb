{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ERxHjKs4qIjC"
   },
   "outputs": [],
   "source": [
    "!pip install nlp\n",
    "!pip install alibi-detect\n",
    "!pip install transformers\n",
    "\n",
    "\n",
    "import nlp\n",
    "import numpy as np\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "from alibi_detect.cd import KSDrift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset: str, split: str = 'test'):\n",
    "    data = nlp.load_dataset(dataset)\n",
    "    X, y = [], []\n",
    "    for x in data[split]:\n",
    "        X.append(x['text'])\n",
    "        y.append(x['label'])\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_dataset('imdb', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample(X: np.ndarray, y: np.ndarray, proba_zero: float, n: int):\n",
    "    if len(y.shape) == 1:\n",
    "        idx_0 = np.where(y == 0)[0]\n",
    "        idx_1 = np.where(y == 1)[0]\n",
    "    else:\n",
    "        idx_0 = np.where(y[:, 0] == 1)[0]\n",
    "        idx_1 = np.where(y[:, 1] == 1)[0]\n",
    "    n_0, n_1 = int(n * proba_zero), int(n * (1 - proba_zero))\n",
    "    n_0\n",
    "    idx_0_out = np.random.choice(idx_0, n_0, replace=False)\n",
    "    idx_1_out = np.random.choice(idx_1, n_1, replace=False)\n",
    "    X_out = np.concatenate([X[idx_0_out], X[idx_1_out]])\n",
    "    y_out = np.concatenate([y[idx_0_out], y[idx_1_out]])\n",
    "    return X_out.tolist(), y_out.tolist()\n",
    "\n",
    "\n",
    "def padding_last(x: np.ndarray, seq_len: int) -> np.ndarray:\n",
    "    try:  # try not to replace padding token\n",
    "        last_token = np.where(x == 0)[0][0]\n",
    "    except:  # no padding\n",
    "        last_token = seq_len - 1\n",
    "    return 1, last_token\n",
    "\n",
    "\n",
    "def padding_first(x: np.ndarray, seq_len: int) -> np.ndarray:\n",
    "    try:  # try not to replace padding token\n",
    "        first_token = np.where(x == 0)[0][-1] + 2\n",
    "    except:  # no padding\n",
    "        first_token = 0\n",
    "    return first_token, seq_len - 1\n",
    "\n",
    "\n",
    "def inject_word(token: int, X: np.ndarray, perc_chg: float, padding: str = 'last'):\n",
    "    seq_len = X.shape[1]\n",
    "    n_chg = int(perc_chg * .01 * seq_len)\n",
    "    X_cp = X.copy()\n",
    "    for _ in range(X.shape[0]):\n",
    "        if padding == 'last':\n",
    "            first_token, last_token = padding_last(X_cp[_, :], seq_len)\n",
    "        else:\n",
    "            first_token, last_token = padding_first(X_cp[_, :], seq_len)\n",
    "        if last_token <= n_chg:\n",
    "            choice_len = seq_len\n",
    "        else:\n",
    "            choice_len = last_token\n",
    "        idx = np.random.choice(np.arange(first_token, choice_len), n_chg, replace=False)\n",
    "        X_cp[_, idx] = token\n",
    "    return X_cp.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1000\n",
    "h0_sample = random_sample(X, Y, proba_zero=.5, n=sample_size)[0]\n",
    "reference_sample = random_sample(X, Y, proba_zero=.5, n=sample_size)[0]\n",
    "imbalance_fractions = [.1, .9]\n",
    "imbalance_samples = {frac: random_sample(X, Y, proba_zero=frac, n=sample_size)[0] for frac in imbalance_fractions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['fantastic', 'good', 'bad', 'horrible']\n",
    "perc_chg = [1., 5.]  # % of tokens to change in an instance\n",
    "\n",
    "words_tf = tokenizer(words)['input_ids']\n",
    "words_tf = [token[1:-1][0] for token in words_tf]\n",
    "max_len = 100\n",
    "tokens = tokenizer(reference_sample, pad_to_max_length=True,\n",
    "                   max_length=max_len, return_tensors='tf')\n",
    "X_word = {}\n",
    "for i, w in enumerate(words_tf):\n",
    "    X_word[words[i]] = {}\n",
    "    for p in perc_chg:\n",
    "        x = inject_word(w, tokens['input_ids'].numpy(), p)\n",
    "        dec = tokenizer.batch_decode(x, **dict(skip_special_tokens=True))\n",
    "        X_word[words[i]][p] = dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alibi_detect.models.pytorch import TransformerEmbedding\n",
    "\n",
    "emb_type = 'hidden_state'\n",
    "n_layers = 8\n",
    "layers = [-_ for _ in range(1, n_layers + 1)]\n",
    "\n",
    "embedding = TransformerEmbedding(model_name, emb_type, layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alibi_detect.cd.tensorflow import UAE\n",
    "\n",
    "tokens = tokenizer(list(X[:5]), pad_to_max_length=True,\n",
    "                   max_length=max_len, return_tensors='tf')\n",
    "x_emb = embedding(tokens)\n",
    "enc_dim = 32\n",
    "shape = (x_emb.shape[1],)\n",
    "\n",
    "uae = UAE(input_layer=embedding, shape=shape, enc_dim=enc_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from alibi_detect.cd.tensorflow import preprocess_drift\n",
    "\n",
    "# define preprocessing function\n",
    "preprocess_fn = partial(preprocess_drift, model=uae, tokenizer=tokenizer,\n",
    "                        max_len=max_len, batch_size=32)\n",
    "\n",
    "# initialize detector\n",
    "drift_detector = KSDrift(X_ref, p_val=.05, preprocess_fn=preprocess_fn, input_shape=(max_len,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_h0 = drift_detector.predict(h0_sample)\n",
    "labels = ['No Drift Detected', 'Drift Detected!']\n",
    "print('Drift: {}'.format(labels[preds_h0['data']['is_drift']]))\n",
    "print('Probability Value: {}'.format(preds_h0['data']['p_val']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in imbalance_samples.items():\n",
    "    preds = drift_detector.predict(v)\n",
    "    print('% Negative Sentiment: {}'.format(k * 100))\n",
    "    print('Drift: {}'.format(labels[preds['data']['is_drift']]))\n",
    "    print('Probability Value: {}'.format(preds['data']['p_val']))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
